https://smartlaboratory.org/ravdess
 - Audio-Visual Database of Emotional Speech and Song 
 - voice actors/scripted lines 
 - labeled calm, happy, sad, angry, fearful, surprise, and disgust expressions

    0 Modality (01 = full-AV, 02 = video-only, 03 = audio-only).
    
    1 Vocal channel (01 = speech, 02 = song).

    2 Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).

    3 Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong  intensity for the ‚Äòneutral‚Äô emotion.

    4 Statement (01 = ‚ÄúKids are talking by the door‚Äù, 02 = ‚ÄúDogs are sitting by the door‚Äù).

    5 Repetition (01 = 1st repetition, 02 = 2nd repetition).

    6 Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).


 https://tspace.library.utoronto.ca/handle/1807/24487
 - Toronto emotional speech set (TESS) Collection
 - 2 actresses
 - recordings portraying 7 emotions
 - anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral
 
 A better way to extract features from audio is to use Mel Frequency Cepstral Coefficients, or MFCCs for short
- http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/

MFCC is used to represent the evnolope of the short time power spectrum cause by the shapening of the vocal tract during certrain vocal inflexions.

    - the periodgram helps determine which frequesncies are present in a frame (20-40ms of sound). alot of this information is not actually needed to ASR 

    - collecting these frames that exagerate create a more accurate representation of the strength of certain frequencies - these is done through mel filter banks

    - 12 MFCC coefficients extracted at every frame 
    - MFCC's help describe large structures of spectrum
    - ignore fine spectral features
    - they are pitch invariant 



wget -r --user=guest2savee --password=welcome! --no-parent http://kahlan.eps.surrey.ac.uk/savee/Data/AudioData/



    frequency domain feature extraction requires the framing
    of audio into overlapping frames/windows on which to perform tansforms on
    these frames are composed of audio samples

    the amplitude envolope the max value across a series of samples

---------------------------------
librosa stuff
- the length of the numpy return is the amount of samples in the audio
- you can pass in the sample_rate as an arg

- the duration of sample is 1/sample_rate


an STFT allows you observe how the different frequency components of the classical Fourier transform evolve over time.
 - moving from a still like image to a video like set of images 
 - create frames and extract a DFT on each frame. 
   - this is known as windowing 
- both window size and frame size are measured in samples
   - window size is the amount of samples that windowing is being applied to

   - the frame size is the amount of samples that are considered on each chunk of the signal to be sent for to STFT

its important that the frame size power of 2 to be able to compute the FFT (since FFT uses logarithmic arithmetic)

the windowing function has an effect on the results of the STFT. such as the Hann window or Mel frequency filter banks.

in an MFCC information pertaining to vocal detail would be stored in the lower end of the 'quefrency' axis, and spectral information would be stored in the higher end of the 'quefrency' axis.

If we assume that humans can perceive frequency down to 20 Hz, this frequency has period 1/20=0.05 [seconds], so two cycles would take 0.1 seconds. If the sampling rate is ùëìùë†=44100 (so that the Nyquist frequency is above the upper limit of audible frequencies of 20000 [Hz]), then a frame length ùëÅùêπ=4096 would have duration ùëÅùêπ/ùëìùë†‚âà0.093 [seconds], which is pretty close to 0.1.


Max pooling is good for picking out edges in the signal. it is however not as good for preserving localized information. considering the location of edges pertain to their freciency we went with Average pooling. an average would give you a strong signal in middle and soft at the edges, leaving you with more information on where the edges of the feature were localized (which is lost with max-pooling)

"We believe that while the max and average functions are rather similar, the use of average pooling encourages the network to identify the complete extent of the object. The basic intuition behind this is that the loss for average pooling benefits when the network identifies all discriminative regions of an object as compared to max pooling" 

http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf

changes in stride produce best results 


https://arxiv.org/pdf/2106.01621v5.pdf

https://www.isca-speech.org/archive/pdfs/interspeech_2017/satt17_interspeech.pdf

https://www.mdpi.com/1424-8220/21/13/4399/pdf

Our model uses the simple rules of CNN architecture, which means that the number of filters must be the same when the same output features map is required. If the size gets cut in half or is reduced up to a half, then the number of filters must be double in the convolution layer to sustain the complexity in each layer.

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7570673/


https://www.researchgate.net/publication/346688712_Dying_ReLU_and_Initialization_Theory_and_Numerical_Examples

https://arxiv.org/pdf/1906.10891.pdf

https://arxiv.org/pdf/1608.04363.pdf

tried to split it between gender and still no accurate

model running leaky relu seems to do well (63_2)

The deep convolutional neural network (CNN) architecture
proposed in this study is comprised of 3 convolutional layers
interleaved with 2 pooling operations, followed by 2 fully connected (dense) 

We apply strided
max-pooling after the first two convolutional layers ` ‚àà {1, 2}
using a stride size equal to the pooling dimensions 
 which reduces the dimensions of the output feature
maps and consequently speeds up training and builds some
scale invariance into the network
layers