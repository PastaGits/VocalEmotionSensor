https://smartlaboratory.org/ravdess
 - Audio-Visual Database of Emotional Speech and Song 
 - voice actors/scripted lines 
 - labeled calm, happy, sad, angry, fearful, surprise, and disgust expressions

    0 Modality (01 = full-AV, 02 = video-only, 03 = audio-only).
    
    1 Vocal channel (01 = speech, 02 = song).

    2 Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).

    3 Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong  intensity for the ‚Äòneutral‚Äô emotion.

    4 Statement (01 = ‚ÄúKids are talking by the door‚Äù, 02 = ‚ÄúDogs are sitting by the door‚Äù).

    5 Repetition (01 = 1st repetition, 02 = 2nd repetition).

    6 Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).


 https://tspace.library.utoronto.ca/handle/1807/24487
 - Toronto emotional speech set (TESS) Collection
 - 2 actresses
 - recordings portraying 7 emotions
 - anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral
 
 A better way to extract features from audio is to use Mel Frequency Cepstral Coefficients, or MFCCs for short
- http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/

MFCC is used to represent the evnolope of the short time power spectrum cause by the shapening of the vocal tract during certrain vocal inflexions.

    - the periodgram helps determine which frequesncies are present in a frame (20-40ms of sound). alot of this information is not actually needed to ASR 

    - collecting these frames that exagerate create a more accurate representation of the strength of certain frequencies - these is done through mel filter banks

    - 12 MFCC coefficients extracted at every frame 
    - MFCC's help describe large structures of spectrum
    - ignore fine spectral features
    - they are pitch invariant 



wget -r --user=guest2savee --password=welcome! --no-parent http://kahlan.eps.surrey.ac.uk/savee/Data/AudioData/



    frequency domain feature extraction requires the framing
    of audio into overlapping frames/windows on which to perform tansforms on
    these frames are composed of audio samples

    the amplitude envolope the max value across a series of samples

---------------------------------
librosa stuff
- the length of the numpy return is the amount of samples in the audio
- you can pass in the sample_rate as an arg

- the duration of sample is 1/sample_rate


an STFT allows you observe how the different frequency components of the classical Fourier transform evolve over time.
 - moving from a still like image to a video like set of images 
 - create frames and extract a DFT on each frame. 
   - this is known as windowing 
- both window size and frame size are measured in samples
   - window size is the amount of samples that windowing is being applied to

   - the frame size is the amount of samples that are considered on each chunk of the signal to be sent for to STFT

its important that the frame size power of 2 to be able to compute the FFT (since FFT uses logarithmic arithmetic)

the windowing function has an effect on the results of the STFT. such as the Hann window or Mel frequency filter banks.

in an MFCC information pertaining to vocal detail would be stored in the lower end of the 'quefrency' axis, and spectral information would be stored in the higher end of the 'quefrency' axis.

If we assume that humans can perceive frequency down to 20 Hz, this frequency has period 1/20=0.05 [seconds], so two cycles would take 0.1 seconds. If the sampling rate is ùëìùë†=44100 (so that the Nyquist frequency is above the upper limit of audible frequencies of 20000 [Hz]), then a frame length ùëÅùêπ=4096 would have duration ùëÅùêπ/ùëìùë†‚âà0.093 [seconds], which is pretty close to 0.1.