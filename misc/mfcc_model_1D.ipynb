{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3f7f5-7af4-4e13-8f85-a524eda1256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "file = h5py.File('E:/dataset_6ms.h5', 'r')\n",
    "#label_dict = {\"['n']\": 0, \"['c']\": 1, \"['h']\": 2, \"['s']\": 3,\n",
    " #             \"['a']\": 4, \"['f']\": 5, \"['d']\": 6, \"['su']\": 7}\n",
    "\n",
    "gender_labels = file['gender'][...].squeeze().astype(str)\n",
    "print('done!')\n",
    "\n",
    "# female_indexes =  np.where(gender_labels == 'female')\n",
    "indexes =  np.where(gender_labels == 'female')\n",
    "print('done!')\n",
    "y = file['label'][indexes]\n",
    "print('done!')\n",
    "X = file['mfcc'][indexes]\n",
    "print('done!')\n",
    "\n",
    "# print('done!')\n",
    "# y = file['label'][relevant_indexes]\n",
    "# print('done!')\n",
    "# X = file['mfcc'][relevant_indexes]\n",
    "# print('done!')\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf971c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract delta and double delta features using librosa\n",
    "X_deltas = np.zeros((X.shape[0], X.shape[1]*3, X.shape[2]))\n",
    "print(X_deltas.shape)\n",
    "for i in range(X.shape[0]):\n",
    "    X_delta = librosa.feature.delta(X[i])\n",
    "    X_delta2 = librosa.feature.delta(X[i], order=2)\n",
    "    # np.vstack all the X, X_delta and X_delta2 \n",
    "    new_features = np.vstack([X[i], X_delta, X_delta2])\n",
    "    X_deltas[i] = new_features\n",
    "    # print(X_deltas[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4caedd-91d9-42f3-9e00-97f1442e9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_deltas, y, test_size=0.33, random_state=32)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01201eaf-9923-4326-a475-b21eb264de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "librosa.display.specshow(X_train[1])\n",
    "plt.show()\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ded05-455b-4685-99c5-383ba3e82b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPooling2D, Add\n",
    "from keras.layers import Flatten, Dropout, LeakyReLU, Permute, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Reshape, AveragePooling2D, Layer\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import adam_v2\n",
    "# import rms prop from keras\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=45)\n",
    "opt = adam_v2.Adam(learning_rate=0.00008)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "\n",
    "feature_shape = X_train[0].shape\n",
    "print(feature_shape)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Reshape(target_shape=(\n",
    "    feature_shape[0], feature_shape[1]), input_shape=(feature_shape[0], feature_shape[1])))\n",
    "# swap the x and y axis\n",
    "model.add(Permute((2, 1)))\n",
    "# model.add(Dense(39, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "\n",
    "# 1st conv layer\n",
    "model.add(Conv1D(16, 5, padding='same'))\n",
    "model.add(Conv1D(16, 5, padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd conv layer\n",
    "model.add(Conv1D(32, 5, padding='same'))\n",
    "model.add(Conv1D(32, 5, padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 3rd conv layer\n",
    "model.add(Conv1D(64, 7, padding='same'))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
    "# flatten the output and add a dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt, metrics=['sparse_categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ae786-ee7a-40ec-93df-8804c98f8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnhistory=model.fit(X_train, y_train, batch_size=1000, epochs=200,validation_data=(X_test, y_test), shuffle=True, callbacks=[es], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfbba3-42d1-434c-a1d9-814c38da5439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model history\n",
    "# cnnhistory.model.save('cnn_66.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db10ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model.save('my_model_984.h5')\n",
    "acc = cnnhistory.history['sparse_categorical_accuracy']\n",
    "val_acc = cnnhistory.history['val_sparse_categorical_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "loss = cnnhistory.history['loss']\n",
    "val_loss = cnnhistory.history['val_loss']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5), dpi=80)\n",
    "\n",
    "ax[0].plot(epochs, acc,'--r', label='Training Acc')\n",
    "ax[0].plot(epochs, val_acc, '-b', label='Validation Acc')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Training and Validation accuracy')\n",
    "\n",
    "ax[1].plot(epochs, loss,'--r' ,label='Training Loss')\n",
    "ax[1].plot(epochs, val_loss, '-g',label='Validation Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Training and Validation Loss')\n",
    "\n",
    "print(\"Model Accuracy Metrics: \")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "labels = ['neutral','calm','happy','sad','angry','fearful']\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "# print(y_pred.argmax(axis=1))\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ConfusionMatrixDisplay.from_predictions(y_pred, y_test, display_labels=labels).plot(ax=ax)\n",
    "print(f\"Accuracy {accuracy_score(y_test, y_pred):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87291ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353fd8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "old = keras.models.load_model('cnn_63.h5')\n",
    "old.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fa698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e9a847315198e1db6c3e64fe9ef1c8f4cc1666ddba368d2a78c3257c3b63773"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
